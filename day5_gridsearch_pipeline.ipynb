{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "146e45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a026c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (353, 10) test: (89, 10)\n"
     ]
    }
   ],
   "source": [
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=10\n",
    ")\n",
    "\n",
    "print(\"train:\", X_train.shape, \"test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ab71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"ridge\", Ridge())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce36f57",
   "metadata": {},
   "source": [
    "# Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a0f8b",
   "metadata": {},
   "source": [
    "// ...existing code...\n",
    "\n",
    "This creates a machine that does two steps in order:\n",
    "\n",
    "- Scale the features (StandardScaler)  \n",
    "- Fit Ridge regression (Ridge)\n",
    "\n",
    "Think: “data goes in → gets standardized → model learns coefficients”.\n",
    "\n",
    "1) Why scaling at all?\n",
    "\n",
    "Your dataset has multiple features (columns). They can be on different “units/scales”.\n",
    "\n",
    "Example (toy):\n",
    "\n",
    "- Feature A: “fat %” ~ 0.1–0.5  \n",
    "- Feature B: “sodium mg” ~ 200–3000  \n",
    "- Feature C: “temperature” ~ 5–50\n",
    "\n",
    "If we fit a model using raw values, the feature with the biggest numerical scale can dominate optimization and regularization behavior.\n",
    "\n",
    "StandardScaler does:\n",
    "\n",
    "For each feature j:\n",
    "\n",
    "$x'_{ij} = \\dfrac{x_{ij} - \\mu_j}{\\sigma_j}$\n",
    "\n",
    "where $\\mu_j$ is the mean of feature $j$ (computed from training data) and $\\sigma_j$ is the standard deviation of feature $j$.\n",
    "\n",
    "After scaling:\n",
    "\n",
    "- every feature has mean ≈ 0  \n",
    "- every feature has standard deviation ≈ 1\n",
    "\n",
    "Intuition: Scaling makes features “comparable”, like converting everything into “how many standard deviations from the average.”\n",
    "\n",
    "2) Why scaling matters specifically for Ridge\n",
    "\n",
    "Ridge regression solves:\n",
    "\n",
    "$\\min_{\\beta}\\; \\dfrac{1}{n}\\|y - X\\beta\\|^2 + \\alpha\\|\\beta\\|^2$\n",
    "\n",
    "The penalty $\\|\\beta\\|^2 = \\sum_{j=1}^p \\beta_j^2$ assumes each coefficient $\\beta_j$ is “measured fairly.” If one feature is 1000× larger than another, coefficients for that feature will be scaled down accordingly, which changes how the penalty affects different features. With scaling, Ridge treats each feature on the same footing.\n",
    "\n",
    "3) Why we need a Pipeline (the leakage reason)\n",
    "\n",
    "Correct evaluation rule: nothing about the test/validation data should influence training.\n",
    "\n",
    "Bad practice (data leakage):\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)   # <-- uses ALL data including test\n",
    "```\n",
    "\n",
    "This uses test data to compute $\\mu$ and $\\sigma$ and can make CV/test scores look overly optimistic.\n",
    "\n",
    "What Pipeline guarantees:\n",
    "\n",
    "```python\n",
    "pipe = Pipeline([(\"scaler\", StandardScaler()), (\"ridge\", Ridge())])\n",
    "```\n",
    "\n",
    "When running cross-validation, scikit-learn will, for each fold:\n",
    "- Fit scaler on training-fold only → compute $\\mu, \\sigma$ from training part  \n",
    "- Transform training-fold using those  \n",
    "- Transform validation-fold using the same scaler (no refit)  \n",
    "- Fit ridge on scaled training-fold  \n",
    "- Evaluate on scaled validation-fold\n",
    "\n",
    "So: no leakage.\n",
    "\n",
    "4) What a Pipeline is doing mathematically\n",
    "\n",
    "You can think of the pipeline as a composed function:\n",
    "\n",
    "$f(x) = \\text{Ridge}(\\text{Scale}(x))$\n",
    "\n",
    "Where Scale(x) = standardized x. Pipeline = “preprocessing + model” treated as one object.\n",
    "\n",
    "5) What happens when you call .fit and .predict\n",
    "\n",
    "pipe.fit(X_train, y_train):\n",
    "- scaler.fit(X_train) → compute $\\mu, \\sigma$  \n",
    "- scaler.transform(X_train) → $X'_{\\text{train}}$  \n",
    "- ridge.fit($X'_{\\text{train}}$, y_train) → learn $\\beta$\n",
    "\n",
    "pipe.predict(X_test):\n",
    "- scaler.transform(X_test) using $\\mu, \\sigma$ from training  \n",
    "- ridge.predict($X'_{\\text{test}}$)\n",
    "\n",
    "Test data is never used to determine scaling.\n",
    "\n",
    "6) Why we left Ridge() without alpha here\n",
    "\n",
    "In Day 5 we will tune $\\alpha$ with GridSearchCV. We will search over $\\alpha \\in \\{10^{-3}, \\dots, 10^{3}\\}$ by passing different values to `ridge__alpha` so they are evaluated fairly.\n",
    "\n",
    "// ...existing code..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c7e8e",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ada5c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': array([1.00000000e-03, 3.16227766e-03, 1.00000000e-02, 3.16227766e-02,\n",
       "        1.00000000e-01, 3.16227766e-01, 1.00000000e+00, 3.16227766e+00,\n",
       "        1.00000000e+01, 3.16227766e+01, 1.00000000e+02, 3.16227766e+02,\n",
       "        1.00000000e+03])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"ridge__alpha\": np.logspace(-3, 3, 13)  # 1e-3 ... 1e+3\n",
    "}\n",
    "param_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde11070",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv = GridSearchCV(\n",
    "    estimator=pipe, #this means we are tuning the pipeline\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",  # maximize \"negative RMSE\"\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\", gscv.best_params_)\n",
    "print(\"Best CV score (neg RMSE):\", gscv.best_score_)\n",
    "print(\"Best CV RMSE:\", -gscv.best_score_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
